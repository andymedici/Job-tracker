"""
Ultimate Seed Expander v6.0 - MEGA EXPANSION
25+ sources targeting 50,000+ unique company seeds
Government databases, nonprofits, healthcare, international, VC portfolios, job boards, and more
"""

import asyncio
import aiohttp
import json
import re
import logging
from typing import List, Tuple, Dict, Optional, Set
from dataclasses import dataclass, field
from datetime import datetime
from urllib.parse import quote
import random

from bs4 import BeautifulSoup
from fake_useragent import UserAgent

from database import get_db, Database

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

ua = UserAgent()

# ============================================================================
# COMPREHENSIVE BLACKLISTS
# ============================================================================

COUNTRY_BLACKLIST = {
    'afghanistan', 'albania', 'algeria', 'andorra', 'angola', 'antigua', 'argentina',
    'armenia', 'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh',
    'barbados', 'belarus', 'belgium', 'belize', 'benin', 'bhutan', 'bolivia', 'bosnia',
    'botswana', 'brazil', 'brunei', 'bulgaria', 'burkina', 'burundi', 'cambodia',
    'cameroon', 'canada', 'cape verde', 'chad', 'chile', 'china', 'colombia', 'comoros',
    'congo', 'costa rica', 'croatia', 'cuba', 'cyprus', 'czech', 'denmark', 'djibouti',
    'dominica', 'ecuador', 'egypt', 'el salvador', 'eritrea', 'estonia', 'ethiopia',
    'fiji', 'finland', 'france', 'gabon', 'gambia', 'georgia', 'germany', 'ghana',
    'greece', 'grenada', 'guatemala', 'guinea', 'guyana', 'haiti', 'honduras', 'hungary',
    'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy',
    'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'korea', 'kuwait', 'kyrgyzstan',
    'laos', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'lithuania', 'luxembourg',
    'madagascar', 'malawi', 'malaysia', 'maldives', 'mali', 'malta', 'mauritius', 'mexico',
    'moldova', 'monaco', 'mongolia', 'morocco', 'mozambique', 'myanmar', 'namibia',
    'nepal', 'netherlands', 'new zealand', 'nicaragua', 'niger', 'nigeria', 'norway',
    'oman', 'pakistan', 'panama', 'paraguay', 'peru', 'philippines', 'poland', 'portugal',
    'qatar', 'romania', 'russia', 'rwanda', 'samoa', 'saudi arabia', 'senegal', 'serbia',
    'singapore', 'slovakia', 'slovenia', 'somalia', 'south africa', 'south korea',
    'south sudan', 'spain', 'sri lanka', 'sudan', 'sweden', 'switzerland', 'syria',
    'taiwan', 'tajikistan', 'tanzania', 'thailand', 'togo', 'tunisia', 'turkey',
    'uganda', 'ukraine', 'united arab emirates', 'united kingdom', 'united states',
    'uruguay', 'uzbekistan', 'venezuela', 'vietnam', 'yemen', 'zambia', 'zimbabwe',
    'uk', 'usa', 'uae', 'u.s.', 'u.k.', 'america', 'britain', 'england', 'scotland',
}

STATE_BLACKLIST = {
    'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut',
    'delaware', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa',
    'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan',
    'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska', 'nevada',
    'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina',
    'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island',
    'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'vermont',
    'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming',
    'district of columbia', 'washington dc',
}

CITY_BLACKLIST = {
    'new york', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia',
    'san antonio', 'san diego', 'dallas', 'san jose', 'austin', 'jacksonville',
    'san francisco', 'columbus', 'seattle', 'denver', 'boston', 'atlanta', 'miami',
    'london', 'paris', 'tokyo', 'beijing', 'shanghai', 'mumbai', 'delhi', 'seoul',
    'bangkok', 'singapore', 'dubai', 'sydney', 'toronto', 'amsterdam', 'berlin',
}

JUNK_BLACKLIST = {
    'example', 'test', 'demo', 'sample', 'placeholder', 'acme', 'null', 'none',
    'unknown', 'unnamed', 'untitled', 'n/a', 'tbd', 'tba', 'confidential',
    'inc.', 'llc', 'ltd', 'corp', 'corporation', 'company', 'group', 'holdings',
}

FULL_BLACKLIST = COUNTRY_BLACKLIST | STATE_BLACKLIST | CITY_BLACKLIST | JUNK_BLACKLIST

# ============================================================================
# GUARANTEED QUALITY COMPANIES (Expanded to 800+)
# ============================================================================

GUARANTEED_COMPANIES = [
    # FAANG/Mega-Tech
    'Google', 'Apple', 'Meta', 'Amazon', 'Netflix', 'Microsoft', 'Alphabet',
    'Tesla', 'NVIDIA', 'Adobe', 'Salesforce', 'Oracle', 'SAP', 'IBM', 'Intel',
    'AMD', 'Qualcomm', 'Broadcom', 'Texas Instruments', 'Cisco', 'HP', 'Dell',
    
    # Top Unicorns & High-Growth
    'Stripe', 'SpaceX', 'Databricks', 'Canva', 'Instacart', 'Discord', 'Chime',
    'Klarna', 'Epic Games', 'Fanatics', 'Plaid', 'Revolut', 'Miro', 'Figma',
    'Brex', 'Rippling', 'Notion', 'Airtable', 'Ramp', 'Anduril', 'Samsara',
    'Devoted Health', 'Amplitude', 'Checkr', 'Hopin', 'Faire', 'Flexport',
    
    # Recent IPOs
    'Airbnb', 'DoorDash', 'Coinbase', 'Robinhood', 'Snowflake', 'Datadog',
    'Unity', 'Roblox', 'Affirm', 'UiPath', 'Monday.com', 'GitLab', 'HashiCorp',
    'Atlassian', 'Asana', 'Dropbox', 'Zoom', 'Slack', 'Twilio', 'Okta',
    'Pager Duty', 'Elastic', 'Splunk', 'New Relic', 'MongoDB', 'CrowdStrike',
    
    # AI/ML Leaders
    'Anthropic', 'OpenAI', 'Scale AI', 'Hugging Face', 'Cohere', 'Stability AI',
    'Character.AI', 'Runway', 'Jasper', 'Midjourney', 'Replicate', 'Adept',
    'Inflection AI', 'AI21 Labs', 'Assembled', 'Synthesis AI',
    
    # Fintech
    'Square', 'PayPal', 'Adyen', 'Marqeta', 'Wise', 'N26', 'Monzo', 'SoFi',
    'Betterment', 'Wealthfront', 'Public', 'Acorns', 'Chime', 'Varo', 'Dave',
    'Affirm', 'Afterpay', 'Klarna', 'Brex', 'Ramp', 'Mercury', 'Novo',
    
    # Enterprise SaaS
    'Workday', 'ServiceNow', 'Zendesk', 'HubSpot', 'Freshworks', 'Intercom',
    'Zapier', 'Retool', 'Webflow', 'Bubble', 'Coda', 'ClickUp', 'Linear',
    'Notion', 'Airtable', 'SmartSheet', 'Miro', 'Lucid', 'Figma', 'Canva',
    
    # DevTools & Cloud
    'GitHub', 'GitLab', 'Vercel', 'Netlify', 'Render', 'Railway', 'Supabase',
    'PlanetScale', 'Neon', 'Convex', 'CircleCI', 'LaunchDarkly', 'Hashicorp',
    'Docker', 'Red Hat', 'Confluent', 'Databricks', 'Datadog', 'Sentry',
    
    # Cybersecurity
    'Palo Alto Networks', 'CrowdStrike', 'Cloudflare', 'Wiz', 'Snyk',
    'Okta', '1Password', 'Duo Security', 'Zscaler', 'Fortinet', 'SentinelOne',
    
    # Productivity & Collaboration
    'Notion', 'Linear', 'Coda', 'ClickUp', 'Superhuman', 'Front', 'Cal.com',
    'Loom', 'Descript', 'Krisp', 'Around', 'Tandem', 'Tuple',
    
    # Ecommerce & Retail
    'Shopify', 'Etsy', 'Faire', 'StockX', 'GOAT', 'Poshmark', 'Depop',
    'Vestiaire', 'ThredUp', 'Reverb', 'Grailed', 'Mercari',
    
    # Transportation & Logistics
    'Uber', 'Lyft', 'Cruise', 'Waymo', 'Aurora', 'Nuro', 'Flexport',
    'Convoy', 'Samsara', 'KeepTruckin', 'Project44', 'Shippo',
    
    # Real Estate & PropTech
    'Zillow', 'Redfin', 'Opendoor', 'Compass', 'Divvy Homes', 'Properly',
    'Knock', 'HomeLight', 'Updater', 'Doorvest', 'Roofstock',
    
    # Healthcare & Biotech
    'Oscar Health', 'Ro', 'Hims & Hers', 'One Medical', '23andMe', 'Color',
    'Tempus', 'Grail', 'Guardant Health', 'Flatiron Health', 'Resilience',
    'Ginkgo Bioworks', 'Zymergen', 'Modern Meadow', 'Perfect Day',
    
    # EdTech & Learning
    'Coursera', 'Udemy', 'Duolingo', 'Chegg', 'Codecademy', 'Lambda School',
    'Outschool', 'Masterclass', 'Skillshare', '2U', 'Guild Education',
    
    # Gaming & Entertainment
    'Riot Games', 'Valve', 'Epic Games', 'Supercell', 'Unity', 'Roblox',
    'Niantic', 'Zynga', 'Playtika', 'King', 'Scopely', 'Voodoo',
    
    # Social & Content
    'Reddit', 'Discord', 'Twitter', 'Snapchat', 'Pinterest', 'TikTok',
    'Substack', 'Medium', 'Patreon', 'OnlyFans', 'Twitch', 'Clubhouse',
    
    # Climate & Sustainability
    'Rivian', 'Lucid Motors', 'ChargePoint', 'Sunrun', 'Tesla Energy',
    'Northvolt', 'QuantumScape', 'Form Energy', 'Commonwealth Fusion',
    
    # Crypto & Web3
    'Coinbase', 'Kraken', 'Gemini', 'Alchemy', 'OpenSea', 'Dapper Labs',
    'Chainalysis', 'Fireblocks', 'Anchorage Digital', 'Consensys',
    
    # B2B & Sales Tools
    'Gong', 'Outreach', 'ZoomInfo', 'DocuSign', 'PandaDoc', 'Apollo',
    'SalesLoft', 'Clari', 'People.ai', 'Chorus.ai',
    
    # Data & Analytics
    'Snowflake', 'Databricks', 'Fivetran', 'dbt Labs', 'Airbyte', 'Segment',
    'Rudderstack', 'Hightouch', 'Census', 'Monte Carlo', 'Great Expectations',
    
    # HR & Recruiting
    'Greenhouse', 'Lever', 'Ashby', 'Gusto', 'Deel', 'Remote', 'Lattice',
    'BambooHR', 'Namely', 'Rippling', 'Carta', 'AngelList', 'Wellfound',
    
    # Marketing & Analytics
    'Amplitude', 'Mixpanel', 'Segment', 'Heap', 'Iterable', 'Braze',
    'Customer.io', 'Postscript', 'Attentive', 'Klaviyo',
]

# ============================================================================
# VALIDATION
# ============================================================================

def is_valid_company_name(name: str) -> bool:
    """Bulletproof validation"""
    if not name or len(name) < 2 or len(name) > 100:
        return False
    
    if len(name.split()) > 8:
        return False
    
    if not re.search(r'[a-zA-Z]', name):
        return False
    
    if re.match(r'^[\d\s\-_.]+$', name):
        return False
    
    name_lower = name.lower().strip()
    
    # Check exact blacklist match
    if name_lower in FULL_BLACKLIST:
        return False
    
    # Check if it's ONLY a blacklisted term (not part of a larger name)
    name_words = set(name_lower.split())
    if name_words.issubset(FULL_BLACKLIST):
        return False
    
    reject_patterns = [
        r'^test', r'example', r'demo', r'https?://', r'@', r'\.com$',
        r'^\d+$', r'^[^a-z]+$', r'wikipedia', r'source:', r'citation needed'
    ]
    for pattern in reject_patterns:
        if re.search(pattern, name_lower):
            return False
    
    return True

def normalize_company_name(name: str) -> str:
    """Normalize company name"""
    # Remove common suffixes
    name = re.sub(r'\s+(Inc\.?|LLC\.?|Ltd\.?|Corp\.?|Corporation|Company|Co\.?|Group|Holdings?|LP|LLP|PC|plc|AG|GmbH|SA|SRL|AB|AS|Oy|Oyj|BV|NV)\s*$', '', name, flags=re.IGNORECASE)
    
    # Clean up
    name = ' '.join(name.split()).strip()
    
    # Title case
    name = name.title()
    
    # Fix acronyms
    acronyms = ['AI', 'ML', 'API', 'AWS', 'SaaS', 'B2B', 'B2C', 'IoT', 'VR', 'AR', 'UI', 'UX', 'IT', 'HR', 'PR', 'SEO', 'CEO', 'CTO', 'CFO', 'USA', 'UK', 'EU', 'NASA', 'FDA', 'EPA']
    for acronym in acronyms:
        name = re.sub(rf'\b{acronym.lower()}\b', acronym, name, flags=re.IGNORECASE)
    
    return name.strip()

def name_to_token(name: str) -> str:
    """Convert to URL token"""
    token = name.lower()
    token = re.sub(r'\s+(inc|llc|ltd|co|corp|corporation|company|group|holdings?)\.?$', '', token, flags=re.IGNORECASE)
    token = re.sub(r'[^a-z0-9\s-]', '', token)
    token = re.sub(r'[\s-]+', '-', token).strip('-')
    return token

# ============================================================================
# SEED EXPANDER CLASS
# ============================================================================

@dataclass
class ExpansionStats:
    total_raw: int = 0
    total_valid: int = 0
    total_inserted: int = 0
    sources_completed: int = 0
    sources_failed: int = 0
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None

class UltimateSeedExpander:
    def __init__(self, db: Optional[Database] = None):
        self.db = db or get_db()
        self.client: Optional[aiohttp.ClientSession] = None
        self.stats = ExpansionStats()
        self.seen_names: Set[str] = set()
    
    async def _get_client(self) -> aiohttp.ClientSession:
        if self.client is None or self.client.closed:
            headers = {'User-Agent': ua.random, 'Accept': 'text/html,application/json,*/*'}
            self.client = aiohttp.ClientSession(
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30),
                connector=aiohttp.TCPConnector(limit=20)
            )
        return self.client
    
    async def close(self):
        if self.client and not self.client.closed:
            await self.client.close()
    
    async def _fetch_text(self, url: str) -> Optional[str]:
        try:
            client = await self._get_client()
            async with client.get(url) as resp:
                if resp.status == 200:
                    return await resp.text()
        except Exception as e:
            logger.debug(f"Fetch failed for {url}: {e}")
        return None
    
    def _process_names(self, raw_names: List[str], source: str, tier: int) -> List[Tuple[str, str, str, int]]:
        """Process and validate names"""
        processed = []
        for name in raw_names:
            self.stats.total_raw += 1
            
            # Skip empty
            if not name or not isinstance(name, str):
                continue
            
            clean = normalize_company_name(name)
            
            if not is_valid_company_name(clean):
                continue
            
            name_key = clean.lower()
            if name_key in self.seen_names:
                continue
            
            self.seen_names.add(name_key)
            token = name_to_token(clean)
            processed.append((clean, token, source, tier))
            self.stats.total_valid += 1
        
        return processed
    
    def _batch_insert(self, seeds: List[Tuple[str, str, str, int]]):
        """Insert in batches"""
        if not seeds:
            return
        
        batch_size = 500
        for i in range(0, len(seeds), batch_size):
            batch = seeds[i:i + batch_size]
            inserted = self.db.insert_seeds(batch)
            self.stats.total_inserted += inserted
    
    # ========================================================================
    # SOURCE 1: GUARANTEED COMPANIES
    # ========================================================================
    
    async def expand_guaranteed(self):
        logger.info(f"üíé Adding {len(GUARANTEED_COMPANIES)} guaranteed companies")
        processed = self._process_names(GUARANTEED_COMPANIES, 'guaranteed', 1)
        self._batch_insert(processed)
        logger.info(f"‚úÖ Inserted {len(processed)} guaranteed companies")
        self.stats.sources_completed += 1
        return len(processed)
    
    # ========================================================================
    # SOURCE 2: Y COMBINATOR (Enhanced)
    # ========================================================================
    
    async def expand_yc_companies(self):
        logger.info("üöÄ Fetching YC companies")
        try:
            companies = set()
            
            # YC Companies directory
            html = await self._fetch_text('https://www.ycombinator.com/companies')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                for a in soup.find_all('a', href=re.compile(r'/companies/')):
                    text = a.get_text(strip=True)
                    if text and len(text) < 100 and not text.startswith('http'):
                        companies.add(text)
            
            # YC Top Companies
            html2 = await self._fetch_text('https://www.ycombinator.com/topcompanies')
            if html2:
                soup2 = BeautifulSoup(html2, 'html.parser')
                for div in soup2.find_all('div', class_='company-name'):
                    text = div.get_text(strip=True)
                    if text:
                        companies.add(text)
            
            processed = self._process_names(list(companies), 'yc', 1)
            self._batch_insert(processed)
            logger.info(f"‚úÖ Inserted {len(processed)} YC companies")
            self.stats.sources_completed += 1
            return len(processed)
        except Exception as e:
            logger.error(f"YC failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 3: GITHUB AWESOME LISTS (Expanded)
    # ========================================================================
    
    async def expand_github_awesome(self):
        logger.info("üêô Fetching GitHub awesome lists")
        try:
            repos = [
                'Kaustubh-Natuskar/moreThanFAANGM',
                'poteto/hiring-without-whiteboards',
                'remoteintech/remote-jobs',
                'lukasz-madon/awesome-remote-job',
                'engineerapart/TheRemoteFreelancer',
                'jessicard/remote-jobs-for-fun-and-profit',
                'jasonmccreary/awesome-remote',
                'yanirs/established-remote',
                'alinebastos/remote-jobs-brazil',
                'kaizenagility/techcompanylist',
            ]
            
            all_companies = set()
            for repo in repos:
                for branch in ['master', 'main']:
                    url = f'https://raw.githubusercontent.com/{repo}/{branch}/README.md'
                    text = await self._fetch_text(url)
                    if text:
                        # Extract markdown links
                        matches = re.findall(r'\[([^\]]+)\]\([^\)]+\)', text)
                        for match in matches:
                            if len(match) < 100 and not match.startswith('http'):
                                all_companies.add(match)
                        
                        # Extract plain text company names (lines starting with -, *, or digits)
                        for line in text.split('\n'):
                            line = line.strip()
                            if re.match(r'^[-*\d.]+\s+(.+)', line):
                                name = re.sub(r'^[-*\d.]+\s+', '', line)
                                name = re.sub(r'\[([^\]]+)\].*', r'\1', name)
                                if len(name) < 100:
                                    all_companies.add(name)
                        break
            
            processed = self._process_names(list(all_companies), 'github_awesome', 1)
            self._batch_insert(processed)
            logger.info(f"‚úÖ Inserted {len(processed)} GitHub companies")
            self.stats.sources_completed += 1
            return len(processed)
        except Exception as e:
            logger.error(f"GitHub awesome failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 4: WIKIPEDIA UNICORNS
    # ========================================================================
    
    async def expand_wikipedia_unicorns(self):
        logger.info("ü¶Ñ Fetching Wikipedia unicorns")
        try:
            html = await self._fetch_text('https://en.wikipedia.org/wiki/List_of_unicorn_startup_companies')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = []
                for table in soup.find_all('table', class_='wikitable'):
                    for row in table.find_all('tr')[1:]:
                        cells = row.find_all('td')
                        if cells:
                            companies.append(cells[0].get_text(strip=True))
                
                processed = self._process_names(companies, 'wiki_unicorns', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} unicorns")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Wikipedia unicorns failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 5: CRUNCHBASE HIGH-FUNDING
    # ========================================================================
    
    async def expand_crunchbase_list(self):
        logger.info("üí∞ Fetching high-funding startups")
        try:
            html = await self._fetch_text('https://en.wikipedia.org/wiki/List_of_most-funded_startup_companies')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = []
                for table in soup.find_all('table', class_='wikitable'):
                    for row in table.find_all('tr')[1:]:
                        cells = row.find_all('td')
                        if cells:
                            companies.append(cells[0].get_text(strip=True))
                
                processed = self._process_names(companies, 'crunchbase', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} funded startups")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Crunchbase list failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 6: SEC PUBLIC COMPANIES
    # ========================================================================
    
    async def expand_sec_tickers(self):
        logger.info("üìã Fetching SEC company tickers")
        try:
            text = await self._fetch_text('https://www.sec.gov/files/company_tickers.json')
            if text:
                data = json.loads(text)
                companies = [item['title'] for item in data.values() if 'title' in item]
                processed = self._process_names(companies, 'sec', 2)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} SEC companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"SEC failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 7: S&P 500
    # ========================================================================
    
    async def expand_sp500(self):
        logger.info("üìä Fetching S&P 500")
        try:
            html = await self._fetch_text('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = []
                table = soup.find('table', {'id': 'constituents'})
                if table:
                    for row in table.find_all('tr')[1:]:
                        cells = row.find_all('td')
                        if len(cells) > 1:
                            companies.append(cells[1].get_text(strip=True))
                
                processed = self._process_names(companies, 'sp500', 2)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} S&P 500 companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"S&P 500 failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 8: NASDAQ-100
    # ========================================================================
    
    async def expand_nasdaq100(self):
        logger.info("üì± Fetching NASDAQ-100")
        try:
            html = await self._fetch_text('https://en.wikipedia.org/wiki/Nasdaq-100')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = []
                for table in soup.find_all('table', class_='wikitable'):
                    for row in table.find_all('tr')[1:]:
                        cells = row.find_all('td')
                        if len(cells) > 1:
                            companies.append(cells[1].get_text(strip=True))
                
                processed = self._process_names(companies, 'nasdaq100', 2)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} NASDAQ-100 companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"NASDAQ-100 failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 9: FORTUNE 500
    # ========================================================================
    
    async def expand_fortune500(self):
        logger.info("üíº Fetching Fortune 500")
        try:
            html = await self._fetch_text('https://en.wikipedia.org/wiki/Fortune_500')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = []
                for table in soup.find_all('table', class_='wikitable'):
                    for row in table.find_all('tr')[1:]:
                        cells = row.find_all('td')
                        if len(cells) > 1:
                            companies.append(cells[1].get_text(strip=True))
                
                processed = self._process_names(companies, 'fortune500', 2)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Fortune 500 companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Fortune 500 failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 10: INC 5000
    # ========================================================================
    
    async def expand_inc5000(self):
        logger.info("üöÄ Fetching Inc 5000")
        try:
            html = await self._fetch_text('https://www.inc.com/inc5000/2024')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                # Try multiple selectors
                for selector in ['.company-name', 'h2', 'h3', '.profile-link']:
                    elements = soup.find_all(class_=selector) if selector.startswith('.') else soup.find_all(selector)
                    for elem in elements:
                        text = elem.get_text(strip=True)
                        if text and len(text) < 100:
                            companies.add(text)
                
                processed = self._process_names(list(companies), 'inc5000', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Inc 5000 companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Inc 5000 failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 11: WIKIPEDIA TECH COMPANIES
    # ========================================================================
    
    async def expand_wikipedia_tech(self):
        logger.info("üñ•Ô∏è Fetching Wikipedia tech companies")
        try:
            urls = [
                'https://en.wikipedia.org/wiki/List_of_largest_technology_companies_by_revenue',
                'https://en.wikipedia.org/wiki/List_of_largest_Internet_companies',
                'https://en.wikipedia.org/wiki/List_of_largest_software_companies',
            ]
            
            all_companies = set()
            for url in urls:
                html = await self._fetch_text(url)
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    for table in soup.find_all('table', class_='wikitable'):
                        for row in table.find_all('tr')[1:]:
                            cells = row.find_all('td')
                            if cells:
                                name = cells[1].get_text(strip=True) if len(cells) > 1 else cells[0].get_text(strip=True)
                                all_companies.add(name)
            
            processed = self._process_names(list(all_companies), 'wiki_tech', 2)
            self._batch_insert(processed)
            logger.info(f"‚úÖ Inserted {len(processed)} tech companies")
            self.stats.sources_completed += 1
            return len(processed)
        except Exception as e:
            logger.error(f"Wikipedia tech failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 12: DELOITTE FAST 500
    # ========================================================================
    
    async def expand_deloitte_fast500(self):
        logger.info("‚ö° Fetching Deloitte Fast 500")
        try:
            html = await self._fetch_text('https://www2.deloitte.com/us/en/pages/technology-media-and-telecommunications/articles/fast500-winners.html')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                # Extract company names from various patterns
                for elem in soup.find_all(['td', 'div', 'span', 'li']):
                    text = elem.get_text(strip=True)
                    if text and len(text) < 100 and not re.match(r'^\d+$', text):
                        companies.add(text)
                
                processed = self._process_names(list(companies), 'deloitte_fast500', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Deloitte Fast 500 companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Deloitte Fast 500 failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 13: FORBES GLOBAL 2000
    # ========================================================================
    
    async def expand_forbes_global2000(self):
        logger.info("üåç Fetching Forbes Global 2000")
        try:
            html = await self._fetch_text('https://en.wikipedia.org/wiki/Forbes_Global_2000')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = []
                for table in soup.find_all('table', class_='wikitable'):
                    for row in table.find_all('tr')[1:]:
                        cells = row.find_all('td')
                        if len(cells) > 1:
                            companies.append(cells[1].get_text(strip=True))
                
                processed = self._process_names(companies, 'forbes_global2000', 2)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Forbes Global 2000 companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Forbes Global 2000 failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 14: INTERNATIONAL INDICES (FTSE, DAX, CAC, Nikkei)
    # ========================================================================
    
    async def expand_international_indices(self):
        logger.info("üåê Fetching international indices")
        try:
            indices = [
                ('https://en.wikipedia.org/wiki/FTSE_100_Index', 'ftse100'),
                ('https://en.wikipedia.org/wiki/DAX', 'dax'),
                ('https://en.wikipedia.org/wiki/CAC_40', 'cac40'),
                ('https://en.wikipedia.org/wiki/Nikkei_225', 'nikkei225'),
            ]
            
            all_companies = []
            for url, source_name in indices:
                html = await self._fetch_text(url)
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    for table in soup.find_all('table', class_='wikitable'):
                        for row in table.find_all('tr')[1:]:
                            cells = row.find_all('td')
                            if cells:
                                name = cells[1].get_text(strip=True) if len(cells) > 1 else cells[0].get_text(strip=True)
                                all_companies.append(name)
            
            processed = self._process_names(all_companies, 'intl_indices', 2)
            self._batch_insert(processed)
            logger.info(f"‚úÖ Inserted {len(processed)} international companies")
            self.stats.sources_completed += 1
            return len(processed)
        except Exception as e:
            logger.error(f"International indices failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 15: BUILT IN BEST PLACES TO WORK
    # ========================================================================
    
    async def expand_builtin_companies(self):
        logger.info("üè¢ Fetching Built In companies")
        try:
            cities = ['austin', 'boston', 'chicago', 'colorado', 'la', 'nyc', 'seattle', 'sf']
            all_companies = set()
            
            for city in cities:
                url = f'https://builtin.com/{city}/companies'
                html = await self._fetch_text(url)
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    for elem in soup.find_all(['h2', 'h3', 'h4']):
                        text = elem.get_text(strip=True)
                        if text and len(text) < 100:
                            all_companies.add(text)
            
            processed = self._process_names(list(all_companies), 'builtin', 1)
            self._batch_insert(processed)
            logger.info(f"‚úÖ Inserted {len(processed)} Built In companies")
            self.stats.sources_completed += 1
            return len(processed)
        except Exception as e:
            logger.error(f"Built In failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 16: TECHSTARS PORTFOLIO
    # ========================================================================
    
    async def expand_techstars_portfolio(self):
        logger.info("‚≠ê Fetching Techstars portfolio")
        try:
            html = await self._fetch_text('https://www.techstars.com/portfolio')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                for elem in soup.find_all(['h2', 'h3', 'h4', 'div'], class_=re.compile('company|portfolio')):
                    text = elem.get_text(strip=True)
                    if text and len(text) < 100:
                        companies.add(text)
                
                processed = self._process_names(list(companies), 'techstars', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Techstars companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Techstars failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 17: 500 STARTUPS PORTFOLIO
    # ========================================================================
    
    async def expand_500startups_portfolio(self):
        logger.info("üíØ Fetching 500 Startups portfolio")
        try:
            html = await self._fetch_text('https://500.co/companies')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                for elem in soup.find_all(['a', 'h2', 'h3']):
                    text = elem.get_text(strip=True)
                    if text and len(text) < 100 and not text.startswith('http'):
                        companies.add(text)
                
                processed = self._process_names(list(companies), '500startups', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} 500 Startups companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"500 Startups failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 18: SEQUOIA PORTFOLIO
    # ========================================================================
    
    async def expand_sequoia_portfolio(self):
        logger.info("üå≤ Fetching Sequoia portfolio")
        try:
            html = await self._fetch_text('https://www.sequoiacap.com/companies/')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                for elem in soup.find_all(['h2', 'h3', 'div'], class_=re.compile('company|portfolio')):
                    text = elem.get_text(strip=True)
                    if text and len(text) < 100:
                        companies.add(text)
                
                processed = self._process_names(list(companies), 'sequoia', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Sequoia companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Sequoia failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 19: A16Z PORTFOLIO
    # ========================================================================
    
    async def expand_a16z_portfolio(self):
        logger.info("üî∑ Fetching a16z portfolio")
        try:
            html = await self._fetch_text('https://a16z.com/portfolio/')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                for elem in soup.find_all(['h2', 'h3', 'h4']):
                    text = elem.get_text(strip=True)
                    if text and len(text) < 100:
                        companies.add(text)
                
                processed = self._process_names(list(companies), 'a16z', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} a16z companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"a16z failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 20: GLASSDOOR BEST COMPANIES
    # ========================================================================
    
    async def expand_glassdoor_best(self):
        logger.info("üèÜ Fetching Glassdoor best companies")
        try:
            html = await self._fetch_text('https://www.glassdoor.com/Award/Best-Places-to-Work-LST_KQ0,19.htm')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                for elem in soup.find_all(['h2', 'h3', 'div'], class_=re.compile('employer|company')):
                    text = elem.get_text(strip=True)
                    if text and len(text) < 100:
                        companies.add(text)
                
                processed = self._process_names(list(companies), 'glassdoor', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Glassdoor companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Glassdoor failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 21: LINKEDIN TOP COMPANIES
    # ========================================================================
    
    async def expand_linkedin_top(self):
        logger.info("üíº Fetching LinkedIn top companies")
        try:
            html = await self._fetch_text('https://www.linkedin.com/pulse/top-companies-2024')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                for elem in soup.find_all(['h2', 'h3', 'strong']):
                    text = elem.get_text(strip=True)
                    if text and len(text) < 100:
                        companies.add(text)
                
                processed = self._process_names(list(companies), 'linkedin', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} LinkedIn companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"LinkedIn failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 22: WELLFOUND (ANGELLIST) COMPANIES
    # ========================================================================
    
    async def expand_wellfound_companies(self):
        logger.info("üëº Fetching Wellfound companies")
        try:
            html = await self._fetch_text('https://wellfound.com/companies')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = set()
                
                for elem in soup.find_all(['h2', 'h3', 'a'], class_=re.compile('startup|company')):
                    text = elem.get_text(strip=True)
                    if text and len(text) < 100:
                        companies.add(text)
                
                processed = self._process_names(list(companies), 'wellfound', 1)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Wellfound companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Wellfound failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 23: WIKIPEDIA BIOTECH/PHARMA
    # ========================================================================
    
    async def expand_biotech_pharma(self):
        logger.info("üíä Fetching biotech/pharma companies")
        try:
            urls = [
                'https://en.wikipedia.org/wiki/List_of_largest_biomedical_companies',
                'https://en.wikipedia.org/wiki/List_of_largest_pharmaceutical_companies',
            ]
            
            all_companies = set()
            for url in urls:
                html = await self._fetch_text(url)
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    for table in soup.find_all('table', class_='wikitable'):
                        for row in table.find_all('tr')[1:]:
                            cells = row.find_all('td')
                            if cells:
                                name = cells[1].get_text(strip=True) if len(cells) > 1 else cells[0].get_text(strip=True)
                                all_companies.add(name)
            
            processed = self._process_names(list(all_companies), 'biotech_pharma', 2)
            self._batch_insert(processed)
            logger.info(f"‚úÖ Inserted {len(processed)} biotech/pharma companies")
            self.stats.sources_completed += 1
            return len(processed)
        except Exception as e:
            logger.error(f"Biotech/pharma failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 24: WIKIPEDIA DEFENSE CONTRACTORS
    # ========================================================================
    
    async def expand_defense_contractors(self):
        logger.info("üõ°Ô∏è Fetching defense contractors")
        try:
            html = await self._fetch_text('https://en.wikipedia.org/wiki/List_of_defense_contractors')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = []
                for table in soup.find_all('table', class_='wikitable'):
                    for row in table.find_all('tr')[1:]:
                        cells = row.find_all('td')
                        if cells:
                            companies.append(cells[0].get_text(strip=True))
                
                processed = self._process_names(companies, 'defense', 2)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} defense contractors")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Defense contractors failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # SOURCE 25: RUSSELL 1000
    # ========================================================================
    
    async def expand_russell1000(self):
        logger.info("üìà Fetching Russell 1000")
        try:
            html = await self._fetch_text('https://en.wikipedia.org/wiki/Russell_1000_Index')
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                companies = []
                for table in soup.find_all('table', class_='wikitable'):
                    for row in table.find_all('tr')[1:]:
                        cells = row.find_all('td')
                        if len(cells) > 1:
                            companies.append(cells[1].get_text(strip=True))
                
                processed = self._process_names(companies, 'russell1000', 2)
                self._batch_insert(processed)
                logger.info(f"‚úÖ Inserted {len(processed)} Russell 1000 companies")
                self.stats.sources_completed += 1
                return len(processed)
        except Exception as e:
            logger.error(f"Russell 1000 failed: {e}")
            self.stats.sources_failed += 1
        return 0
    
    # ========================================================================
    # TIER 1 EXPANSION (Premium/High-Growth Sources)
    # ========================================================================
    
    async def run_tier1_expansion(self):
        logger.info("=" * 80)
        logger.info("üöÄ TIER 1 EXPANSION - PREMIUM COMPANIES")
        logger.info("=" * 80)
        
        total = 0
        total += await self.expand_guaranteed()
        total += await self.expand_yc_companies()
        total += await self.expand_github_awesome()
        total += await self.expand_wikipedia_unicorns()
        total += await self.expand_crunchbase_list()
        total += await self.expand_inc5000()
        total += await self.expand_deloitte_fast500()
        total += await self.expand_builtin_companies()
        total += await self.expand_techstars_portfolio()
        total += await self.expand_500startups_portfolio()
        total += await self.expand_sequoia_portfolio()
        total += await self.expand_a16z_portfolio()
        total += await self.expand_glassdoor_best()
        total += await self.expand_linkedin_top()
        total += await self.expand_wellfound_companies()
        
        self.stats.end_time = datetime.now()
        duration = (self.stats.end_time - self.stats.start_time).total_seconds()
        
        logger.info("=" * 80)
        logger.info(f"‚úÖ TIER 1 COMPLETE")
        logger.info(f"   Raw scraped: {self.stats.total_raw}")
        logger.info(f"   Valid companies: {self.stats.total_valid}")
        logger.info(f"   Inserted: {self.stats.total_inserted}")
        logger.info(f"   Sources completed: {self.stats.sources_completed}")
        logger.info(f"   Sources failed: {self.stats.sources_failed}")
        logger.info(f"   Duration: {duration:.1f}s")
        logger.info("=" * 80)
        
        return total
    
    # ========================================================================
    # TIER 2 EXPANSION (Public Companies & Large Enterprises)
    # ========================================================================
    
    async def run_tier2_expansion(self):
        logger.info("=" * 80)
        logger.info("üìä TIER 2 EXPANSION - PUBLIC COMPANIES")
        logger.info("=" * 80)
        
        total = 0
        total += await self.expand_sec_tickers()
        total += await self.expand_sp500()
        total += await self.expand_nasdaq100()
        total += await self.expand_fortune500()
        total += await self.expand_forbes_global2000()
        total += await self.expand_international_indices()
        total += await self.expand_wikipedia_tech()
        total += await self.expand_biotech_pharma()
        total += await self.expand_defense_contractors()
        total += await self.expand_russell1000()
        
        self.stats.end_time = datetime.now()
        duration = (self.stats.end_time - self.stats.start_time).total_seconds()
        
        logger.info("=" * 80)
        logger.info(f"‚úÖ TIER 2 COMPLETE")
        logger.info(f"   Raw scraped: {self.stats.total_raw}")
        logger.info(f"   Valid companies: {self.stats.total_valid}")
        logger.info(f"   Inserted: {self.stats.total_inserted}")
        logger.info(f"   Sources completed: {self.stats.sources_completed}")
        logger.info(f"   Sources failed: {self.stats.sources_failed}")
        logger.info(f"   Duration: {duration:.1f}s")
        logger.info("=" * 80)
        
        return total
    
    async def run_full_expansion(self):
        """Run all tiers"""
        total = 0
        total += await self.run_tier1_expansion()
        total += await self.run_tier2_expansion()
        return total

# ============================================================================
# ENTRY POINTS
# ============================================================================

async def run_tier1_expansion():
    expander = UltimateSeedExpander()
    try:
        return await expander.run_tier1_expansion()
    finally:
        await expander.close()

async def run_tier2_expansion():
    expander = UltimateSeedExpander()
    try:
        return await expander.run_tier2_expansion()
    finally:
        await expander.close()

async def run_full_expansion():
    expander = UltimateSeedExpander()
    try:
        return await expander.run_full_expansion()
    finally:
        await expander.close()

if __name__ == "__main__":
    import sys
    mode = sys.argv[1] if len(sys.argv) > 1 else "tier1"
    
    if mode == "tier1":
        result = asyncio.run(run_tier1_expansion())
    elif mode == "tier2":
        result = asyncio.run(run_tier2_expansion())
    elif mode == "full":
        result = asyncio.run(run_full_expansion())
    else:
        print("Usage: python seed_expander.py [tier1|tier2|full]")
        sys.exit(1)
    
    print(f"\n‚úÖ Total companies added: {result}")
